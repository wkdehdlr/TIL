# [아파치 카프카 애플리케이션 프로그래밍] 개념부터 컨슈머, 프로듀서, 커넥트, 스트림즈까지!

## 카프카 기본 개념 설명
### 카프카 생태계
![스크린샷 2022-06-19 오후 5.11.26.png](https://user-images.githubusercontent.com/26949623/175778002-e23ae907-7b05-469b-a020-fca234ec7e0c.png)
### 카프카 브로커와 클러스터
![스크린샷 2022-06-19 오후 5.18.45.png](https://user-images.githubusercontent.com/26949623/175778006-590c3763-2619-4137-af7f-29a2687df501.png)
- 서버 한대에 브로커 한대 운영
- 브로커 3대 이상을 클러스터로 묶어서 운영
### 카프카 클러스터와 주키퍼
![스크린샷 2022-06-19 오후 5.22.12.png](https://user-images.githubusercontent.com/26949623/175778009-ff8583cc-ae32-4707-b470-48969e2c7ecb.png)
- 카프카 클러스터를 실행하기 위해서는 `주키퍼`가 필요
- 주키퍼 앙상블을 통해 여러 카프카 클러스터 운영가능
- 주키퍼의 서로 다른 `znode`에 클러스터를 지정
- root znode에 각 클러스터별 znode를 생성하고 클러스터 실행시 root가 아닌 하위 znode로 설정
- 카프카 3.0 부터는 주키퍼 없이도 클러스터 동작 가능
### 카프카 브로커의 역할
- 컨트롤러
    - 클러스터의 여러 브로커 중 한 대가 컨트롤러 역할
    - 다른 브로커들의 상태를 체크 > 브로커가 클러스터에서 빠지는 경우, 해당 브로커에 존재하는 리더 파티션을 나머지 브로커로 재분배
    - 컨트롤러 역할을 하는 브로커에 장애가 생기면 다른 브로커가 컨트롤러 역할
- 데이터 삭제
    - 컨슈머나 프로듀서가 데이터 삭제를 요청할 수 없다
    - 오직 브로커만이 데이터 삭제 가능
    - 데이터 삭제는 파일 단위(`로그 세그먼트`) > 세그먼트는 일반적인 데이터베이스처럼 특정 데이터를 선별해서 삭제 불가
- 컨슈머 오프셋 저장
    - `__consumer_offsets` 토픽에 오프셋 저장
    - 이 토픽은 자동으로 생성
- 그룹 코디네이터
    - 리밸런싱
    - 컨슈머 그룹의 상태를 체크하고 파티션을 컨슈머와 매칭되도록 분배
    - 컨슈머 그룹에서 컨슈머가 빠지면 매칭되지 않은 파티션을 정상 동작하는 컨슈머로 할당
### 브로커 로그와 세그먼트
- 데이터 저장
    - config/server.properties > log.dir 옵션에 정의한 디렉토리에 데이터 저장
    - `토픽 이름-파티션 번호` 조합으로 하위 디렉토리 생성해서 데이터 저장
        ```shell
        $ ls /tmp/kafka-logs
        __consumer_offsets-0    __consumer_offsets-21
        ...
        
        >> hello.kafka 토픽의 0번 파티션
        $ ls /tmp/kafka-logs/hello.kafka-0
        0000000000000000.index  0000000000000000.log
        0000000000000000.timeindex  leader-epoch-checkpoint
        ```
        - log
            - 메시지와 메타데이터
        - index
            - 메시지의 오프셋을 인덱싱한 정보
        - timeindex
            - 메시지에 포함된 timestamp 값을 기준으로 인덱싱한 정보
- 세그먼트
```shell
$ ls /tmp/kafka-logs/hello.kafaka-0
0000000000000000.log
0000000000000010.log
0000000000000020.log
```
![스크린샷 2022-06-19 오후 6.00.45.png](https://user-images.githubusercontent.com/26949623/175778014-611fbc1c-142c-4477-9850-9af4eee2d2c7.png)
- `log.segment.bytes`
    - 바이트 단위의 최대 세그먼트 크기 지정. 기본값 1GB
- `log.roll.ms(hours)`
    - 세그먼트가 신규 생성된 이후 다음 파일로 넘어가는 시간 주기. 기본값은 7일
    - 최대 세그먼트가 다 안차도 주기가 되면 다음으로 넘어감
- 액티브 세그먼트는 삭제 대상에서 제외
- 액티브 세그먼트가 아닌 일반 세그먼트는 `retention` 옵션에 따라 삭제 대상으로 지정
### 세그먼트와 삭제주기
- `cleanup.policy=delete`
    - retention.ms(minutes, hours) : 세그먼트를 보유할 최대 기간. 기본값 7일
    - retention.bytes : 파티션당 로그 적재 바이트 값. 기본값은 -1(지정하지 않음)
    - log.retention.check.interval.ms : 세그먼트가 삭제 영역에 들어왔는지 확인하는 간격. 기본값은 5분
- `cleanup.policy=compact`
![스크린샷 2022-06-19 오후 6.24.34.png](https://user-images.githubusercontent.com/26949623/175778015-69201be9-b090-408d-a91d-8020f6994a34.png)
    - 동일 메시지 키에 대해 오래된 데이터를 삭제
    - 액티브 세그먼트를 제외한 세그먼트가 대상
    
    ![스크린샷 2022-06-19 오후 6.32.12.png](https://user-images.githubusercontent.com/26949623/175778018-c7b3fe31-6af8-4310-9a69-cfffc3f6f0de.png)
    - 테일 : 압축 정책에 의해 압축이 완료된 레코드. 클린로그
    - 헤드 : 압축 정칙애 되기 전 레코드. 더티로그
    
    ![스크린샷 2022-06-19 오후 6.35.03.png](https://user-images.githubusercontent.com/26949623/175778019-69cd5101-cc92-4c76-93df-b2f068e0dc18.png)
    - `min.cleanable.dirty.ratio`
        - 액티브 세그먼트를 제외한 세그먼트에 남아있는 테일 영역 레코드 수와 헤드 영역 레코드 수의 비율
### 복제(replication)
![스크린샷 2022-06-19 오후 8.44.11.png](https://user-images.githubusercontent.com/26949623/175778021-bc10ffda-c2fe-440c-bae3-e30c62698f53.png)
- `replication factor(복제 개수)`
    - min : 1(복제없음) , max : 브로커개수
- 프로듀서, 컨슈머는 only `리더` 파티션과 통신
- 팔로워 파티션들은 리더 파티션과 오프셋 차이를 확인하여 데이터를 복제
- 리더 파티션이 장애로 다운되면 팔로워 파티션 중 하나가 리더로 승급
### ISR(In-Sync-Replicas)
> 리더 파티션과 팔로워 파티션 모두 싱크된 상태

![스크린샷 2022-06-19 오후 8.57.00.png](https://user-images.githubusercontent.com/26949623/175778022-de3131e8-b318-4753-bb94-c3a0c68715fb.png)

> 팔로워 파티션이 리더 파티션 데이터를 모두 복제하지 못한 상태에서<br>
> 리더 파티션이 다운되어 팔로우 파티션이 리더로 승급되면<br>
> 데이터 유실이 발생할 수 있다

![스크린샷 2022-06-19 오후 8.57.10.png](https://user-images.githubusercontent.com/26949623/175778023-78c74ae9-a3ff-4bc3-8b16-02ff92b4c587.png)
- `unclean.leader.election.enable`=true : 유실을 감수함. 복제가 안된 팔로워 파티션을 리더로 승급
- `unclean.leader.election.enable`=false : 유실을 감수하지 않음. 리더 파티션이 복구될 때까지 대기

> 복제로 인해 데이터 저장 개수 차이가 발생할 수 있는데 <br>
> 컨슈머는 `min.insync.replicas` 설정값 이상으로 복제된 레코드를 가져간다

![스크린샷 2022-06-19 오후 8.57.20.png](https://user-images.githubusercontent.com/26949623/175778025-3644ab55-5a00-4eea-9269-4d87aa83367e.png)
- `하이 워터마크`
    - min.insync.replicas 개수 만큼 복제가 완료되어 컨슈머가 가져갈 수 있는 레코드의 오프셋 번호
### 토픽과 파티션
![스크린샷 2022-06-19 오후 9.16.55.png](https://user-images.githubusercontent.com/26949623/175778026-b32c82f4-044d-4124-a846-431db6b963fe.png)
- 토픽은 1개 이상의 파티션을 소유
  
![스크린샷 2022-06-19 오후 9.17.13.png](https://user-images.githubusercontent.com/26949623/175778028-8f70b697-d72e-4adc-b411-69300181b4b7.png)
- 토픽을 생성할 때, `round-robin` 방식으로 각 브로커에 리더 파티션이 할당
    - 여러 브로커와 골고루 네트워크 통신

![스크린샷 2022-06-19 오후 9.17.30.png](https://user-images.githubusercontent.com/26949623/175778029-72a26e69-62e5-4744-85c0-8787d8978221.png)
- 특정 브로커에 리더 파티션이 볼리면 `kafka-reassign-partition.sh` 으로 파티션 재분배

![스크린샷 2022-06-19 오후 9.17.40.png](https://user-images.githubusercontent.com/26949623/175778030-7e3fba3d-2eb9-41a9-add9-45535092822c.png)
- 파티션 : 컨슈머 = 1 : 1 :white_check_mark:
- 파티션 : 컨슈머 = N : 1 :white_check_mark:
- 파티션 : 컨슈머 = 1 : N :x:
    - 파티션 개수는 그대로인데 컨슈머만 늘린다고 처리량이 늘어나지 않는다

![스크린샷 2022-06-19 오후 9.17.47.png](https://user-images.githubusercontent.com/26949623/175778031-b3a4acdf-510e-4c54-bb9a-cf496d6b8716.png)
- 파티션을 줄이는것은 불가능
### 레코드
![스크린샷 2022-06-19 오후 9.56.21.png](https://user-images.githubusercontent.com/26949623/175778032-5b180797-24b2-47bf-ad31-a4315075a9d2.png)
- timestamp
    - `message.timestamp.type` 토픽 단위로 설정가능
        - CreateTime(생성시간), 기본값
        - LogAppendTime(브로커 적재 시간)
- offset
    - 브로커에 적재될 때 오프셋 지정
        - 프로듀서가 생성한 레코드에는 없는 정보
    - 파티션별로 고유값
- headers
    - key/value
- key
    - 파티셔너에 따라 토픽의 파티션 번호가 정해짐
    - 기본값 : null > `round-robin`
    - 해쉬값
- value
### 클라이언트 메타데이터
> 카프카 클라이언트는 통신하고자 하는 리더 파티션의 위치를 알기 위해 데이터를 주고 받기 전에 메타데이터를 브로커로부터 전달받는다 

![스크린샷 2022-06-19 오후 10.07.05.png](https://user-images.githubusercontent.com/26949623/175778345-f7ca9490-4733-46ec-8f71-bce00e3ea051.png)
- `metadata.max.age.ms` : 메타데이터 강제 리프레쉬. 기본값 5분
- `metadata.max.idle.ms` : 프로듀서가 유휴상태일 경우, 메타데이터 캐시 유지 기간. 기본값 5분

> 카프카 클라이언트는 반드시 리더 파티션과 통신해야 한다<br>
> 잘못된 브로커로 통신하면 `LEADER_NOT_AVAILABLE` 발생

![스크린샷 2022-06-19 오후 10.07.26.png](https://user-images.githubusercontent.com/26949623/175778346-5c1125db-ac42-42ce-8052-37ba5c72044a.png)
## 카프카 프로듀서 개발
### 프로듀서 내부 구조
![스크린샷 2022-06-25 오후 9 19 39](https://user-images.githubusercontent.com/26949623/175773188-d8c329aa-0963-47ef-9bf9-39fdab39f79d.png)
- ProducerRecord : 프로듀서에서 생성하는 레코드. 오프셋은 미포함(카프카 클러스터에 적재되는 시점에 할당)
- Partitioner : 레코드를 어느 파티션으로 전송할지 지정하는 역할. 기본값 : `DefaultPartitioner`
- Accumulator : 배치처리를 위해 레코드를 모으는 버퍼
### 파티셔너
- UniformStickyPartitioner : 기본값(카프카 클라이언트 2.5.0 버전에서)
- RoundRobinPartitioner

- 메시지 키가 있는 경우 :white_check_mark:
    - 메시지 키의 해시값을 이용해 전송
    - 동일한 메시지 키를 갖는 레코드는 동일한 파티션으로 전달
    - 만약 파티션 개수가 변경될 경우, 메시지 키와 파티션 번호 매칭이 깨짐 > 파티션 개수조정 필요가 없도록 처음부터 넉넉하게 설정
- 메시지 키가 없는 경우 :x:
    - 최대한 동일하게 분배
    - UniformStickyPartitioner는 RoundRobinPartitioner의 단점을 개선
    - RoundRobinPartitioner
        - 레코드가 들어오는 대로 파티션을 순회하면서 전송
        - Accumulator에서 묶이는 정도가 적기 때문에 전송 성능이 낮음
    - UniformStickyPartitioner
        - Accumulator에서 레코드들이 배치로 묶일 때까지 기다렸다가 전송
        - 배치로 묶일 뿐 파티션을 순회하는건 같음
- 커스텀 파티셔너
    - `Partitioner` 인터페이스를 재정의
### 프로듀서 주요 옵션(필수 옵션)
> 필수옵션은 기본값이 없다. 따라서 세팅을 반드시 해야한다. 안하면 어플리케이션이 안뜸
- bootstrap.servers
    - 프로듀서가 데이터를 전송할 카프카 클러스터에 속한 브로커의 `호스트:포트`
    - 일반적으로 상용환경에서 2개 이상 브로커 정보를 입력하여 일부 브로커에 이슈가 발생하더라도 이슈가 없도록 설정
- key.serializer
    - `메시지 키`를 직렬화하는 클래스
- value.serializer
    - `메시지 값`을 직렬화하는 클래스
### 프로듀서 주요 옵션(선택 옵션)
- acks
    - 프로듀서가 전송한 데이터가 `브로커들`에 정상적으로 저장되었는지 전송 성공 여부를 확인. 기본값 1
    - 0, 1, -1
- linger.ms
    - 배치를 전송하기 전까지 기다리는 최소 시간. 기본값 0
- retries
    - 브로커로부터 에러를 받고 난 뒤 재전송을 시도하는 횟수. 기본값 2147483647
- max.in.flight.request.per.connection
    - 브로커에 한 번에 요청하는 최대 커넥션 개수. 설정된 값만큼 동시에 브로커에 전달 요청을 수행. 기본값 5
- partitoner.class
- enable.idempotence
    - 멱등성 프로듀서로 동작할지 여부. 
    - 기본값
        - 2.5 : false
        - 3.x : true
- transactional.id
    - 프로듀서가 레코드를 전송할 때 레코드를 트랜잭션 단위로 묶을지 여부. 기본값 null
    - 설정하면 enable.idempotence는 자동으로 true
### ISR과 acks
> replication.factor=2

![스크린샷 2022-06-25 오후 10 27 48](https://user-images.githubusercontent.com/26949623/175775580-a27d9170-7192-4f81-9762-1f480ad395de.png)
- acks=0
    - sender로 보내면 끝
    - 리더 파티션으로 데이터가 저장되었는지 확인하지 않는다
    
![스크린샷 2022-06-25 오후 10 27 57](https://user-images.githubusercontent.com/26949623/175775583-f4e11232-5346-411d-9362-6a309a6a9e64.png)
- acks=1(대부분의 환경에서 운영가능)
    - 리더 파티션에만 정상적으로 적재되었는지 확인
    - 리더 파티션에 정상적으로 적재되지 않았다면 리더 파티션에 적재될 때까지 재시도 가능
    - 여전히 데이터 유실 가능성있음
        - 리더 파티션에 적재가 완료되었지만 팔로워 파티션에서 아직 동기화되기 전에 리더파티션에서 장애가 발생하는 경우

![스크린샷 2022-06-25 오후 10 28 08](https://user-images.githubusercontent.com/26949623/175775584-b4de898c-7937-4e7f-934a-8361651ec66a.png)
- acks=-1(all)
    - 리더 파티션뿐만 아니라 팔로워 파티션까지 데이터가 적재되었는지 확인
    - 신뢰도가 가장 높다 > 처리량은 매우 낮다(속도는 거의 포기하는 수준)
    - 토픽 단위로 설정 가능한 `min.insync.replicas`에 따라서 팔로워 파티션 적재 확인
        - replication.factor가 3이고 min.insync.replicas가 2이면 팔로워 파티션 1개까지만 동기화 확인
## 카프카 컨슈머 개발
### 컨슈머 내부 구조
![컨슈머](https://user-images.githubusercontent.com/26949623/175805392-e74c297e-7953-4904-9a95-f3e8048b144a.png)
- Fetcher : 리더 파티션으로부터 레코드들을 미리 가져와서 대기(배치로 가져옴)
- poll() : Fetcher에 있는 레코드들을 리턴
- ConsumerRecords :처리하고자 하는 레코드들의 모음. 오프셋 포함
### 컨슈머 그룹
![consumer2](https://user-images.githubusercontent.com/26949623/175805857-099b869a-50b4-40ec-8fcc-00b91213170b.png)
- 기본적으로 컨슈머가 토픽을 구독하게 되면, 토픽의 전체 파티션에 대해서 데이터를 가져감

![consumer3](https://user-images.githubusercontent.com/26949623/175805884-d4a8da4b-69a5-4835-a15b-b39b89578d47.png)
- 컨슈머 그룹의 컨슈머가 파티션 개수보다 많을 경우
- 남는 컨슈머는 `유휴 상태`

![consumer5](https://user-images.githubusercontent.com/26949623/175805890-a00957f9-2270-4fd6-81c1-a7effa563800.png)
![consumer4](https://user-images.githubusercontent.com/26949623/175805892-15814587-9b08-4eb8-87f4-8ceedda4b56d.png)
- 컨슈머 그룹을 활용하는 이유
- 효율적인 데이터 파이프라인을 운영하기 위함
### 리밸런싱
> 일부 컨슈머에 장애가 발생하면, 장애가 발생한 컨슈머에 할당된 파티션은 장애가 발생하지 않은 컨슈머에 소유권이 넘어간다

![리밸런싱](https://user-images.githubusercontent.com/26949623/175806040-11ac07c7-0571-4c62-9f1c-d055b4be022e.png)
- 컨슈머가 `추가`되는경우
- 컨슈머가 `제외`되는경우
- RebalancingListener
    - 컨슈머가 데이터를 처리하는 도중에 언제든지 발생할 수 있으므로, 데이터 처리 중 발생한 리밸런싱에 대응하는 코드를 작성해야 한다
### 커밋
> 카프카 브로커 내부 토픽(__consumer_offsets)에 컨슈머가 데이터를 어디까지 가져갔는지 기록

![커밋](https://user-images.githubusercontent.com/26949623/175806151-46c37b62-6b75-4f99-a922-24196e684d88.png)
### 어사이너
> 컨슈머와 파티션 할당 정책. 기본값 RangeAssignor(2.5.0 버전)

- RangeAssignor : 각 토픽에서 파티션을 숫자로 정렬, 컨슈머를 사전순서로 정렬 후 할당
- RoundRobinAssignor : 모든 파티션을 컨슈머에서 번갈아가면서 할당
- StickyAssignor : 최대한 파티션을 균등하게 배분
- assign vs subscribe
    - assign은 특정 `파티션`에 컨슈머 할당
    - subscribe는 `컨슈머 그룹`에 컨슈머 할당
### 컨슈머 주요 옵션(필수 옵션)
> 필수옵션은 기본값이 없다. 따라서 세팅을 반드시 해야한다. 안하면 어플리케이션이 안뜸

- bootstrap.servers
    - 컨슈머가 데이터를 가져올 카프카 클러스터에 속한 브로커의 `호스트:포트`
    - 일반적으로 상용환경에서 2개 이상 브로커 정보를 입력하여 일부 브로커에 이슈가 발생하더라도 이슈가 없도록 설정
- key.serializer
    - `메시지 키`를 직렬화하는 클래스
- value.serializer
    - `메시지 값`을 직렬화하는 클래스
### 컨슈머 주요 옵션(선택 옵션)
- group.id
    - subscribe() 메서드로 토픽을 구독할 때 필수
    - 기본값 null
- auto.offset.reset
    - 컨슈머 그룹이 특정 파티션을 읽을 때 저장된 컨슈머 오프셋이 없는 경우, 어느 오프셋부터 읽을지 선택
    - 이미 컨슈머 오프셋이 있다면 이 옵션은 무시
    - latest, earliest, none
    - 기본값 latest
    - 새로운 컨슈머 투입할 때 고려
- enable.auto.commit
    - 기본값 true
- auto.commit.interval.ms
    - 자동 커밋일 경우 오프셋 커밋 간격
    - 기본값 5000(5초)
- max.poll.records
    - poll() 메서드를 통해 반환되는 레코드 개수
    - 기본값 500
- max.poll.interval.ms
    - poll() 메서드 호출 간격 최대 시간
    - 브로커 입장에서 poll() 호출 후 해당 시간 동안 poll() 호출이 없으면 컨슈머와 끊어짐
    - 기본값 300000(5분)
- session.timeout.ms
    - 컨슈머가 브로커와 연결이 끊기는 최대 시간
    - 브로커 입장에서 heartbeat 수신 이후 해당 시간 동안 heartbeat가 수신되지 않으면 컨슈머와 연결이 끊어짐
    - 기본값 10000(10초)
- heartbeat.interval.ms
    - 하트비트를 전송하는 시간 간격
    - 기본값 3000(3초)
- isolation.level
    - 트랜잭션 프로듀서가 레코드를 트랜잭션 단위로 보낼 경우 사용
### 수동 커밋 컨슈머
- 동기 오프셋 커밋
    - `commitSync()`
    ```java
    while(true) {
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));
        for (ConsumerRecord<String, String> record : records) {
            logger.info("record : {}", record );
        }
        consumer.commitSync();
    }    
    ```
    - 레코드 단위
    ```java
    while(true) {
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));
        Map<TopicPartiton, OffsetAndMetadata> currentOffset = new HashMap<>();
  
        for (ConsumerRecord<String, String> record : records) {
            logger.info("record : {}", record );
            currentOffset.put(
                    new TopicPartition(record.topic(), record.partition()),
                    new OffsetAndMetadata(record.offset() + 1, null));
            consumer.commitSync(currentOffset);
        }
    }    
    ```

 - 비동기 오프셋 커밋
    - `commitAsync()`
    - 콜백 설정 가능
    ```java
    while (true) {
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));
        for (ConsumerRecord<String, String> record : records) {
            logger.info("record:{}", record);
        }
        
        consumer.commitAsync(new OffsetCommitCallback() {
            public void onComplete(Map<TopicPartition, OffsetAndMetadata> offsets, Exception e) {
                if (e != null)
                    System.err.println("Commit failed");
                else 
                    System.out.println("Commit succeeded");
                
                if (e != null)
                    logger.error("Commit failed for offsets {}", offsets, e);
            }
        });
    }
    ```
### 리밸런스 리스너
- `ConsumerRebalanceListener`
    - onPartitonAssigned() : 리밸런스가 끝난 뒤에 파티션 할당이 완료되면 호출
    - onPartitionRevoked() : 리밸런스가 시작되기 직전에 호출 > 
### 컨슈머의 안전한 종료
> 정상적으로 종료되지 않은 컨슈머는 세션 타임아웃이 발생할 때까지 컨슈머 그룹에 좀비로 남게 된다

- 컨슈머를 안전하게 종료하기 위해 KafkaConsumer 클래스는 `wakeup()` 메서드를 지원
```java
Runtime.getRuntime().addShutdownHook(new ShutdownThread());
 ...
 consumer = new KafkaConsumer<>(configs);
 consumer.subscribe(Arrays.asList(TOPIC_NAME));
 try {
     while (true) {
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));
        for (ConsumerRecord<String, String> record : records) {
            logger.info("{}", record);
        }
    }
 } catch (WakeupException e) {
    logger.warn("Wakeup consumer");
 } finally {
    consumer.close();
 }
```
### 멀티스레드 컨슈머
![컨슈머](https://user-images.githubusercontent.com/26949623/175809258-91e2b896-3298-4f39-9c32-c294c8eaeaae.png)
### 컨슈머 랙
> 파티션의 최신 오프셋과 컨슈머 오프셋 간 차이

![컨슈머](https://user-images.githubusercontent.com/26949623/175809262-9cf3f332-ee65-464b-8eec-1d3cddd739f0.png)
- 컨슈머 랙은 컨슈머 그룹, 토픽, 파티션별로 생성
- 1개 토픽에 3개의 파티션이 있고 1개의 컨슈머 그룹이면 컨슈머 랙은 총 3개
## 멱등성
- at least once
- at most once
- exactly once
### 멱등성 프로듀서
> 데이터 중복 적재를 막고자 `enable.idempotence` 옵션을 사용하여 `exactly once`를 지원
- 카프카 3.0.0 부터 `enable.idempotence` 기본값 true, acks=all 도 지정됨
- 멱등성 프로듀서는 정확히 한번 브로커에 데이터를 적재하기 위해 정말로 한번만 전송하는것은 아니다.
- 상황에 따라 retry가 발생할 수 있는데, 이 때 브로커가 중복을 판단
- 멱등성 프로듀서가 아닌 경우 :x:
![멱등성프로듀서X](https://user-images.githubusercontent.com/26949623/175810076-a7184984-43fe-4b7f-a1cf-652ae895aadc.png)
- 멱등성 프로듀서 경우 :white_check_mark:
![멱등성프로듀서O](https://user-images.githubusercontent.com/26949623/175810080-1b0ac311-802a-44f2-9f4b-8269a8418400.png)
### 멱등성 프로듀서 동작방식
> 기본 프로듀서와 달리 데이터를 브로커로 전달할 때 `프로듀서 PID`와 `레코드의 시퀀스 넘버 SID`를 함께 전달<br>
> 브로커는 이 값을 기준으로 멱등성 처리
### 멱등성 프로듀서 한계
- 프로듀서가 이슈 발생으로 비정상 종료되어 재시작되면 PID가 달라지기 때문에, 동일한 데이터를 보낸경우 브로커는 다른 데이터로 인식한다
### 트랜잭션 프로듀서와 컨슈머
![트랜잭션](https://user-images.githubusercontent.com/26949623/175810272-f5a0383c-1faf-4fe6-8832-0d0b3ca25a6c.png)
- 트랜잭션 프로듀서 설정
    - `transaction.id` 설정
        ```java
        configs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, UUID.randomUUID());
        Producer<String, String> producer = new KafkaProducer<>(configs);
      
        producer.initTransactions();
        producer.beginTransaction();
      
        producer.send(new ProducerRecord<>(TOPIC, "전달하는 메시지 값"));
      
        producer.commitTransaction();
        producer.close();
        ```
- 트랜잭션 컨슈머 설정
    - `isolation.level=read_committed` 설정
        ```java
        configs.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(configs);
        ```
